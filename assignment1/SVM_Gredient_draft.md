# 这是SVM loss function数学的推导

SVM loss function
$$
L=\frac{1}{n}\sum^n_i\sum_{j\neq i}^nmax(0,S_j-S_i+1)+\lambda\sum_i{w_i^2}
$$
> 这里的$S_i$的代指方式极度弱智，S默认为在当前的图像数据，即$X_i$与$W$相乘得到的结果，为一个1xn或者nx1的向量。$S_i$指的是第i个位置上的标量。

那么对于一个$w_k$来说，其偏导为
$$
\frac{\part{L}}{\part{w_k}}
=
\frac{1}{n}\sum^n_i\sum_{j\neq i}^nmax(0,\frac{\part(S_j-S_i+1)}{\part w_k}
+
2\lambda{w_k}
$$

如果矩阵化实现，怎么提取$\frac{\part(S_j-S_i+1)}{\part w_k}$将会是一件很烦人的事情，所幸我们先完善的是svm_loss_naive版本，所以我们可以一边进行loop一边运算梯度。

到这个时候，第二个问题出现了，我们在loop中进行运算时采用的机制，使用伪代码表述的话是

- 对于每一个图片$X_i$，计算它用w所得的score,即计算出
  $$
  S_1,S_2,S_3,S_4,S_5....S_C     
  $$
  其中$C$指的是class的数量, 具体的来说的话，$S_1=X_i·C_{[1,:]}$

- 然后每项减去$S_i$的score，再加一，得到查看其是否大于1。

  - 若是，则加上$S_j-S_i+1$，
  - 不然则不做处理(+0)

结合我们的偏导公式的前半部分，即
$$
\frac{1}{n}\sum^n_i\sum_{j\neq i}^nmax(0,\frac{\part(S_j-S_i+1)}{\part w_k})
$$
,我们可以发现,当我们每做一步$S_j-S_i+1$这个操作的时候，其对应的**所有**$w_k$的偏导都要对这个计算求偏导，求出的值即为$X_i$在第$k$个位置上出现的值，那我就把它叫做$X_{[i][k]}$吧。

那么也就是说，我们可以把对$w_k$依次做求偏导的过程替换为每做一次运算，求出在这次用到的所有的$w_k$因为这次运算而增加的数值，对于$S_j$来说，是对每个$C_{[:,j]}$求对应增加的导数，即为$dW_{[:,j]}+X_i$，对于$S_i$来说，是对每个$C_{[:,i]}$求对应的导数，即为$dW_{[:,i]}+X_i$

